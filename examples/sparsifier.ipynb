{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sparsifier Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we introduce the `Sparsifier` class and demonstrate its basic functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sparseklearn import Sparsifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the sparsifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_sparsifier` will apply the transform $HD$ to $X$, draw the `num_feat_comp` indices we will keep for each row of $HDX$, and then discard the data. As an example, we'll generate a dataset $X \\in \\mathbb{R}^{10 \\times 5}$, where each row is a datapoint and there are 5 features. We'll sparsify $X$ into 3 dimensions, so that we are keeping $RHDX \\in \\mathbb{R}^{10 \\times 3}$.\n",
    "\n",
    "To use the sparsifier we need to specify the number of features of the original space (`num_feat_full`), the number of samples (`num_samp`), and the number of features we wish to keep in the compressed setting (`num_feat_comp`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.random.RandomState(12)\n",
    "num_samp, num_feat_full = 10, 5\n",
    "X = rs.rand(num_samp, num_feat_full)\n",
    "num_feat_comp = 3\n",
    "sp = Sparsifier(num_feat_full = num_feat_full, num_feat_comp = num_feat_comp, num_samp = num_samp, random_state = rs, transform = None)\n",
    "sp.fit_sparsifier(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mask` attribute indicates which indices are kept for each datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RHDX` attribute contains the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.RHDX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we didn't use a transform in the Sparsifier, `RHDX` is just a subset of `X`. Check this for an arbitrary datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 3\n",
    "(X[row][sp.mask[row]] == sp.RHDX[row]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on sparsified data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of sparsifying the data is to permit operations on mixtures of sparse datapoints and dense statistics. Having fit the `Sparsifier` object, we can discard $X$ and use built-in methods to compute things like distances and covariances. We can compute the (approximate) pairwise distances between all the datapoints in the sample, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.pairwise_distances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the pairwise distances between the compressed data and a set of dense datapoints, which might be statistics we wish to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_statistics = np.random.rand(4,num_feat_full)\n",
    "sp.pairwise_distances(dense_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sparsifier` class contains several other built-in methods to compute related quantities, including:\n",
    "- `weighted_means`\n",
    "- `weighted_means_and_variances`\n",
    "- `pairwise_mahalanobis_distances`\n",
    "See the docstrings for more information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sp.pairwise_mahalanobis_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Johnson-Lindenstrauss Lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Johnson-Lindenstrauss Lemma guarantees the existence of a projection $\\Omega: \\mathbb{R}^P \\to \\mathbb{R}^Q$ such that pairwise distances on a fixed number of points are preserved within a low tolerance with high probability. The Sparsifier is a random projection that acts like the JL projections. We will see an example of this here. We will first embed low-dimensional data into a higher dimension with some noise, and then see that as we keep more and more components, the error in the pairwise distances shrinks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ortho_group\n",
    "from sklearn.metrics import pairwise_distances, mean_squared_error\n",
    "original_latent_dimension = 20\n",
    "embedded_dimension = 100\n",
    "num_samp = 100\n",
    "rs = np.random.RandomState(73)\n",
    "X = rs.rand(num_samp, original_latent_dimension) # generate the low-dimensional data\n",
    "X = np.concatenate((X, np.zeros([num_samp,embedded_dimension-original_latent_dimension])), axis=1) # embed into higher dimension\n",
    "X += rs.normal(scale = 1e-8, size = (num_samp, embedded_dimension)) # add noise\n",
    "X = X[:, rs.permutation(np.arange(0,embedded_dimension,1))] # shuffle columns\n",
    "true_distances = pairwise_distances(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = Sparsifier(num_samp = num_samp, num_feat_full = embedded_dimension, num_feat_comp = 10)\n",
    "sp.fit_sparsifier(X=X)\n",
    "HDX = sp.apply_HD(X)\n",
    "\n",
    "num_feat_comp_array = np.arange(1,original_latent_dimension*3)\n",
    "mse = np.zeros([3, len(num_feat_comp_array)])\n",
    "for i, num_feat_comp in enumerate(num_feat_comp_array):\n",
    "    \n",
    "    sp1 = Sparsifier(num_samp = num_samp, num_feat_full = embedded_dimension, num_feat_comp = num_feat_comp)\n",
    "    sp1.fit_sparsifier(HDX=HDX)\n",
    "    mse[0,i] = mean_squared_error(true_distances, sp1.pairwise_distances())\n",
    "    \n",
    "    sp2 = Sparsifier(num_samp = num_samp, num_feat_full = embedded_dimension, num_feat_comp = num_feat_comp, num_feat_shared = num_feat_comp//10 + 1)\n",
    "    sp2.fit_sparsifier(HDX=HDX)\n",
    "    mse[1,i] = mean_squared_error(true_distances, sp2.pairwise_distances())\n",
    "    \n",
    "    sp3 = Sparsifier(num_samp = num_samp, num_feat_full = embedded_dimension, num_feat_comp = num_feat_comp, num_feat_shared = num_feat_comp)\n",
    "    sp3.fit_sparsifier(HDX=HDX)\n",
    "    mse[2,i] = mean_squared_error(true_distances, sp3.pairwise_distances())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(num_feat_comp_array, mse[0], label = 'No shared features', linewidth = 2)\n",
    "ax.plot(num_feat_comp_array, mse[1], label = '~10% shared features', linewidth = 2)\n",
    "ax.plot(num_feat_comp_array, mse[2], label = '100% shared features', linewidth = 2)\n",
    "ax.set_ylabel('MSE', fontsize = 12)\n",
    "ax.set_xlabel(r'Number of features kept, $Q$', fontsize = 12)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the MSE of the pairwise distances as a function of the number of features we keep out of 100 original features. The fact that we resample the projection $\\mathbf{R}_i$ for each datapoint is how the algorithms can be one-pass, but as a consequence we lose accuracy compared to the classical case when computing statitics on two compressed points, because we approximate these statitics using information in the indices shared by both points:\n",
    "\n",
    "\\begin{equation}\n",
    "||\\mathbf{x} - \\mathbf{y}||^2 \\simeq \\sum_{j \\in \\mathbf{R}_x \\cap \\mathbf{R}_y} (x_j - y_j)^2\n",
    "\\end{equation}\n",
    "\n",
    "Under high compression it's likely that $\\mathbf{R}_x \\cap \\mathbf{R}_y = \\emptyset$, where we use $\\mathbf{R}_x \\cap \\mathbf{R}_y$ to denote the intersection of features preserved under both projections. Keeping the number of features constant, accuracy increases as the number of these forced to be shared between all datapoints is increased. To mitigate this we can force some number features to be shared across all datapoints. We did so in the above using the `num_feat_shared` parameter when we initialized the sparsifier.\n",
    "\n",
    "In our algorithms we typically do not need to compute such quantities (in fact, to date the only time we do is during `k-means++` initialization, where we sample a new point with probability proportional to its distance from the current set of initial means). Instead we perform computations like $||\\mathbf{R}_i^T \\mathbf{x}_i - \\mathbf{R}_i^T \\boldsymbol{\\mu}||$ where $\\boldsymbol{\\mu}$ is dense. In this case we no longer suffer from the problem described above, since we can use every feature preserved by $\\mathbf{R}_i$ rather than an intersection with some other projection.  \n",
    "\n",
    "We show an example of this below, where we use the same data as above, but compute the pairwise distances between a few dense points and the rest of the sparsified data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_vectors, preconditioned_dense_vectors = X[:10], HDX[:10]\n",
    "X_new, HDX_new = X[10:], HDX[10:]\n",
    "true_distances = pairwise_distances(X_new, Y=dense_vectors)\n",
    "num_samp = X_new.shape[0]\n",
    "num_feat_comp_array = np.arange(1,original_latent_dimension*3)\n",
    "mse = np.zeros([3, len(num_feat_comp_array)])\n",
    "for i, num_feat_comp in enumerate(num_feat_comp_array):\n",
    "    \n",
    "    sp1 = Sparsifier(num_samp = num_samp, num_feat_full = embedded_dimension, num_feat_comp = num_feat_comp)\n",
    "    sp1.fit_sparsifier(HDX=HDX_new)\n",
    "    mse[0,i] = mean_squared_error(true_distances, sp1.pairwise_distances(Y=preconditioned_dense_vectors))\n",
    "    \n",
    "    sp2 = Sparsifier(num_samp = num_samp, num_feat_full = embedded_dimension, num_feat_comp = num_feat_comp, num_feat_shared = num_feat_comp//10 + 1)\n",
    "    sp2.fit_sparsifier(HDX=HDX_new)\n",
    "    mse[1,i] = mean_squared_error(true_distances, sp2.pairwise_distances(Y=preconditioned_dense_vectors))\n",
    "    \n",
    "    sp3 = Sparsifier(num_samp = num_samp, num_feat_full = embedded_dimension, num_feat_comp = num_feat_comp, num_feat_shared = num_feat_comp)\n",
    "    sp3.fit_sparsifier(HDX=HDX_new)\n",
    "    mse[2,i] = mean_squared_error(true_distances, sp3.pairwise_distances(Y=preconditioned_dense_vectors))\n",
    "    \n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(num_feat_comp_array, mse[0], label = 'No shared features', linewidth = 2)\n",
    "ax.plot(num_feat_comp_array, mse[1], label = '~10% shared features', linewidth = 2)\n",
    "ax.plot(num_feat_comp_array, mse[2], label = '100% shared features', linewidth = 2)\n",
    "ax.set_ylabel('MSE', fontsize = 12)\n",
    "ax.set_xlabel(r'Number of features kept, $Q$', fontsize = 12)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there is now no advantage to using more shared features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
