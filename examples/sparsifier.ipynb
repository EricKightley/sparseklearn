{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sparsifier Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we introduce the `Sparsifier` class and demonstrate its basic functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sparseklearn import Sparsifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the sparsifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_sparsifier` will apply the transform $HD$ to $X$, draw the `num_feat_comp` indices we will keep for each row of $HDX$, and then discard the data. As an example, we'll generate a dataset $X \\in \\mathbb{R}^{10 \\times 5}$, where each row is a datapoint and there are 5 features. We'll sparsify $X$ into 3 dimensions, so that we are keeping $RHDX \\in \\mathbb{R}^{10 \\times 3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.random.RandomState(12)\n",
    "num_samp, num_feat_full = 10, 5\n",
    "X = rs.rand(num_samp, num_feat_full)\n",
    "num_feat_comp = 3\n",
    "sp = Sparsifier(num_feat_full = num_feat_full, num_feat_comp = num_feat_comp, num_samp = num_samp, random_state = rs, transform = None)\n",
    "sp.fit_sparsifier(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mask` attribute indicates which indices are kept for each datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RHDX` attribute contains the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.RHDX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we didn't use a transform in the Sparsifier, `RHDX` is just a subset of `X`. Check this for an arbitrary datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 3\n",
    "(X[row][sp.mask[row]] == sp.RHDX[row]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on sparsified data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of sparsifying the data is to permit operations on mixtures of sparse datapoints and dense statistics. Having fit the `Sparsifier` object, we can discard $X$ and use built-in methods to compute things like distances and covariances. We can compute the (approximate) pairwise distances between all the datapoints in the sample, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.pairwise_distances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the pairwise distances between the compressed data and a set of dense datapoints, which might be statistics we wish to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_statistics = np.random.rand(4,num_feat_full)\n",
    "sp.pairwise_distances(dense_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sparsifier` class contains several other built-in methods to compute related quantities, including:\n",
    "- `weighted_means`\n",
    "- `weighted_means_and_variances`\n",
    "- `pairwise_mahalanobis_distances`\n",
    "See the docstrings for more information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sp.pairwise_mahalanobis_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Johnson-Lindenstrauss Lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Johnson-Lindenstrauss Lemma guarantees the existence of a projection $\\Omega: \\mathbb{R}^P \\to \\mathbb{R}^Q$ such that pairwise distances on a fixed number of points are preserved within a low tolerance with high probability. The Sparsifier is a random projection that acts like the JL projections. We will see an example of this here. We will first embed low-dimensional data into a higher dimension with some noise, and then see that as we keep more and more components, the error in the pairwise distances shrinks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ortho_group\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "original_latent_dimension = 20\n",
    "embedded_dimension = 100\n",
    "num_samp = 100\n",
    "rs = np.random.RandomState(73)\n",
    "X = rs.rand(num_samp, original_latent_dimension) # generate the low-dimensional data\n",
    "X = np.concatenate((X, np.zeros([num_samp,embedded_dimension-original_latent_dimension])), axis=1) # embed into higher dimension\n",
    "X += rs.normal(scale = 1e-8, size = (num_samp, embedded_dimension)) # add noise\n",
    "X = X[:, rs.permutation(np.arange(0,embedded_dimension,1))] # shuffle columns\n",
    "true_distances = distance.squareform(distance.pdist(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = Sparsifier(num_samp = num_samp, num_feat_full = embedded_dimension, num_feat_comp = 10)\n",
    "sp.fit_sparsifier(X=X)\n",
    "HDX = sp.apply_HD(X)\n",
    "\n",
    "mse = np.zeros([3, len(num_feat_comp_array)])\n",
    "num_feat_comp_array = np.arange(1,original_latent_dimension*3)\n",
    "for i, num_feat_comp in enumerate(num_feat_comp_array):\n",
    "    \n",
    "    sp1 = Sparsifier(num_samp = num_samp, num_feat_full = embedded_dimension, num_feat_comp = num_feat_comp)\n",
    "    sp1.fit_sparsifier(HDX=HDX)\n",
    "    mse[0,i] = mean_squared_error(true_distances, sp1.pairwise_distances())\n",
    "    \n",
    "    sp2 = Sparsifier(num_samp = num_samp, num_feat_full = embedded_dimension, num_feat_comp = num_feat_comp, num_feat_shared = num_feat_comp//10 + 1)\n",
    "    sp2.fit_sparsifier(HDX=HDX)\n",
    "    mse[1,i] = mean_squared_error(true_distances, sp2.pairwise_distances())\n",
    "    \n",
    "    sp3 = Sparsifier(num_samp = num_samp, num_feat_full = embedded_dimension, num_feat_comp = num_feat_comp, num_feat_shared = num_feat_comp)\n",
    "    sp3.fit_sparsifier(HDX=HDX)\n",
    "    mse[2,i] = mean_squared_error(true_distances, sp3.pairwise_distances())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(num_feat_comp_array, mse[0], label = 'No shared features', linewidth = 2)\n",
    "ax.plot(num_feat_comp_array, mse[1], label = '~10% shared features', linewidth = 2)\n",
    "ax.plot(num_feat_comp_array, mse[2], label = '100% shared features', linewidth = 2)\n",
    "ax.set_ylabel('MSE', fontsize = 12)\n",
    "ax.set_xlabel(r'Number of features kept, $Q$', fontsize = 12)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
